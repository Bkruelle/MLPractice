{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms as TF\n",
    "from torchvision import datasets\n",
    "import torchvision.models as models\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODIFIED CITYSCAPES DATASET: ORIGINALLY FROM TORCHVISION SOURCE CODE GITHUB\n",
    "#https://github.com/pytorch/vision/blob/master/torchvision/datasets/cityscapes.py\n",
    "\n",
    "#For Cityscapes Dataset\n",
    "import json\n",
    "import os\n",
    "from collections import namedtuple\n",
    "import zipfile\n",
    "#from .utils import extract_archive, verify_str_arg, iterable_to_str\n",
    "#from .vision import VisionDataset\n",
    "from PIL import Image\n",
    "\n",
    "#For VisionDataset\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "#For Util Functions\n",
    "import os.path\n",
    "import hashlib\n",
    "import gzip\n",
    "import errno\n",
    "import tarfile\n",
    "from torch.utils.model_zoo import tqdm\n",
    "\n",
    "#For added albumentation augmentations\n",
    "from albumentations import (GaussianBlur, RandomFog, HorizontalFlip, RandomSnow, Compose, OneOf)\n",
    "\n",
    "def extract_archive(from_path, to_path=None, remove_finished=False):\n",
    "    if to_path is None:\n",
    "        to_path = os.path.dirname(from_path)\n",
    "\n",
    "    if _is_tar(from_path):\n",
    "        with tarfile.open(from_path, 'r') as tar:\n",
    "            tar.extractall(path=to_path)\n",
    "    elif _is_targz(from_path) or _is_tgz(from_path):\n",
    "        with tarfile.open(from_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=to_path)\n",
    "    elif _is_tarxz(from_path):\n",
    "        with tarfile.open(from_path, 'r:xz') as tar:\n",
    "            tar.extractall(path=to_path)\n",
    "    elif _is_gzip(from_path):\n",
    "        to_path = os.path.join(to_path, os.path.splitext(os.path.basename(from_path))[0])\n",
    "        with open(to_path, \"wb\") as out_f, gzip.GzipFile(from_path) as zip_f:\n",
    "            out_f.write(zip_f.read())\n",
    "    elif _is_zip(from_path):\n",
    "        with zipfile.ZipFile(from_path, 'r') as z:\n",
    "            z.extractall(to_path)\n",
    "    else:\n",
    "        raise ValueError(\"Extraction of {} not supported\".format(from_path))\n",
    "\n",
    "    if remove_finished:\n",
    "        os.remove(from_path)\n",
    "        \n",
    "def iterable_to_str(iterable):\n",
    "    return \"'\" + \"', '\".join([str(item) for item in iterable]) + \"'\"\n",
    "\n",
    "\n",
    "def verify_str_arg(value, arg=None, valid_values=None, custom_msg=None):\n",
    "    if not isinstance(value, torch._six.string_classes):\n",
    "        if arg is None:\n",
    "            msg = \"Expected type str, but got type {type}.\"\n",
    "        else:\n",
    "            msg = \"Expected type str for argument {arg}, but got type {type}.\"\n",
    "        msg = msg.format(type=type(value), arg=arg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    if valid_values is None:\n",
    "        return value\n",
    "\n",
    "    if value not in valid_values:\n",
    "        if custom_msg is not None:\n",
    "            msg = custom_msg\n",
    "        else:\n",
    "            msg = (\"Unknown value '{value}' for argument {arg}. \"\n",
    "                   \"Valid values are {{{valid_values}}}.\")\n",
    "            msg = msg.format(value=value, arg=arg,\n",
    "                             valid_values=iterable_to_str(valid_values))\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    return value\n",
    "\n",
    "class VisionDataset(data.Dataset):\n",
    "    _repr_indent = 4\n",
    "\n",
    "    def __init__(self, root, transforms=None, transform=None, target_transform=None):\n",
    "        if isinstance(root, torch._six.string_classes):\n",
    "            root = os.path.expanduser(root)\n",
    "        self.root = root\n",
    "\n",
    "        has_transforms = transforms is not None\n",
    "        has_separate_transform = transform is not None or target_transform is not None\n",
    "        if has_transforms and has_separate_transform:\n",
    "            raise ValueError(\"Only transforms or transform/target_transform can \"\n",
    "                             \"be passed as argument\")\n",
    "\n",
    "        # for backwards-compatibility\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "        if has_separate_transform:\n",
    "            transforms = StandardTransform(transform, target_transform)\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        head = \"Dataset \" + self.__class__.__name__\n",
    "        body = [\"Number of datapoints: {}\".format(self.__len__())]\n",
    "        if self.root is not None:\n",
    "            body.append(\"Root location: {}\".format(self.root))\n",
    "        body += self.extra_repr().splitlines()\n",
    "        if hasattr(self, \"transforms\") and self.transforms is not None:\n",
    "            body += [repr(self.transforms)]\n",
    "        lines = [head] + [\" \" * self._repr_indent + line for line in body]\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "class StandardTransform(object):\n",
    "    def __init__(self, transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __call__(self, input, target):\n",
    "        if self.transform is not None:\n",
    "            input = self.transform(input)\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return input, target\n",
    "\n",
    "    def _format_transform_repr(self, transform, head):\n",
    "        lines = transform.__repr__().splitlines()\n",
    "        return ([\"{}{}\".format(head, lines[0])] +\n",
    "                [\"{}{}\".format(\" \" * len(head), line) for line in lines[1:]])\n",
    "\n",
    "    def __repr__(self):\n",
    "        body = [self.__class__.__name__]\n",
    "        if self.transform is not None:\n",
    "            body += self._format_transform_repr(self.transform,\n",
    "                                                \"Transform: \")\n",
    "        if self.target_transform is not None:\n",
    "            body += self._format_transform_repr(self.target_transform,\n",
    "                                                \"Target transform: \")\n",
    "\n",
    "        return '\\n'.join(body)\n",
    "\n",
    "def albumAug(p = 0.5):\n",
    "    return Compose([\n",
    "        GaussianBlur(blur_limit=7, always_apply=False, p=p),\n",
    "        OneOf([\n",
    "            RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, alpha_coef=0.08, always_apply=False, p=p),\n",
    "            RandomSnow(snow_point_lower=0.1, snow_point_upper=0.3, brightness_coeff=2.5, always_apply=False, p=p),\n",
    "        ], p=0.5),\n",
    "        HorizontalFlip(always_apply=False, p=p),\n",
    "    ], p=1)\n",
    "    \n",
    "class Cityscapes(VisionDataset):\n",
    "    \"\"\"`Cityscapes <http://www.cityscapes-dataset.com/>`_ Dataset.\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where directory ``leftImg8bit``\n",
    "            and ``gtFine`` or ``gtCoarse`` are located.\n",
    "        split (string, optional): The image split to use, ``train``, ``test`` or ``val`` if mode=\"fine\"\n",
    "            otherwise ``train``, ``train_extra`` or ``val``\n",
    "        mode (string, optional): The quality mode to use, ``fine`` or ``coarse``\n",
    "        target_type (string or list, optional): Type of target to use, ``instance``, ``semantic``, ``polygon``\n",
    "            or ``color``. Can also be a list to output a tuple with all specified target types.\n",
    "        transform (callable, optional): A function/transform that takes in a PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n",
    "            and returns a transformed version.\n",
    "    Examples:\n",
    "        Get semantic segmentation target\n",
    "        .. code-block:: python\n",
    "            dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
    "                                 target_type='semantic')\n",
    "            img, smnt = dataset[0]\n",
    "        Get multiple targets\n",
    "        .. code-block:: python\n",
    "            dataset = Cityscapes('./data/cityscapes', split='train', mode='fine',\n",
    "                                 target_type=['instance', 'color', 'polygon'])\n",
    "            img, (inst, col, poly) = dataset[0]\n",
    "        Validate on the \"coarse\" set\n",
    "        .. code-block:: python\n",
    "            dataset = Cityscapes('./data/cityscapes', split='val', mode='coarse',\n",
    "                                 target_type='semantic')\n",
    "            img, smnt = dataset[0]\n",
    "    \"\"\"\n",
    "\n",
    "    # Based on https://github.com/mcordts/cityscapesScripts\n",
    "    CityscapesClass = namedtuple('CityscapesClass', ['name', 'id', 'train_id', 'category', 'category_id',\n",
    "                                                     'has_instances', 'ignore_in_eval', 'color'])\n",
    "\n",
    "    classes = [\n",
    "        CityscapesClass('unlabeled', 0, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('ego vehicle', 1, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('out of roi', 3, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('static', 4, 255, 'void', 0, False, True, (0, 0, 0)),\n",
    "        CityscapesClass('dynamic', 5, 255, 'void', 0, False, True, (111, 74, 0)),\n",
    "        CityscapesClass('ground', 6, 255, 'void', 0, False, True, (81, 0, 81)),\n",
    "        CityscapesClass('road', 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n",
    "        CityscapesClass('sidewalk', 8, 1, 'flat', 1, False, False, (244, 35, 232)),\n",
    "        CityscapesClass('parking', 9, 255, 'flat', 1, False, True, (250, 170, 160)),\n",
    "        CityscapesClass('rail track', 10, 255, 'flat', 1, False, True, (230, 150, 140)),\n",
    "        CityscapesClass('building', 11, 2, 'construction', 2, False, False, (70, 70, 70)),\n",
    "        CityscapesClass('wall', 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n",
    "        CityscapesClass('fence', 13, 4, 'construction', 2, False, False, (190, 153, 153)),\n",
    "        CityscapesClass('guard rail', 14, 255, 'construction', 2, False, True, (180, 165, 180)),\n",
    "        CityscapesClass('bridge', 15, 255, 'construction', 2, False, True, (150, 100, 100)),\n",
    "        CityscapesClass('tunnel', 16, 255, 'construction', 2, False, True, (150, 120, 90)),\n",
    "        CityscapesClass('pole', 17, 5, 'object', 3, False, False, (153, 153, 153)),\n",
    "        CityscapesClass('polegroup', 18, 255, 'object', 3, False, True, (153, 153, 153)),\n",
    "        CityscapesClass('traffic light', 19, 6, 'object', 3, False, False, (250, 170, 30)),\n",
    "        CityscapesClass('traffic sign', 20, 7, 'object', 3, False, False, (220, 220, 0)),\n",
    "        CityscapesClass('vegetation', 21, 8, 'nature', 4, False, False, (107, 142, 35)),\n",
    "        CityscapesClass('terrain', 22, 9, 'nature', 4, False, False, (152, 251, 152)),\n",
    "        CityscapesClass('sky', 23, 10, 'sky', 5, False, False, (70, 130, 180)),\n",
    "        CityscapesClass('person', 24, 11, 'human', 6, True, False, (220, 20, 60)),\n",
    "        CityscapesClass('rider', 25, 12, 'human', 6, True, False, (255, 0, 0)),\n",
    "        CityscapesClass('car', 26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n",
    "        CityscapesClass('truck', 27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n",
    "        CityscapesClass('bus', 28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n",
    "        CityscapesClass('caravan', 29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n",
    "        CityscapesClass('trailer', 30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n",
    "        CityscapesClass('train', 31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n",
    "        CityscapesClass('motorcycle', 32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n",
    "        CityscapesClass('bicycle', 33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n",
    "        CityscapesClass('license plate', -1, -1, 'vehicle', 7, False, True, (0, 0, 142)),\n",
    "    ]\n",
    "\n",
    "    def __init__(self, root, split='train', mode='fine', target_type='instance',\n",
    "                 transform=None, target_transform=None, transforms=None):\n",
    "        super(Cityscapes, self).__init__(root, transforms, transform, target_transform)\n",
    "        self.mode = 'gtFine' if mode == 'fine' else 'gtCoarse'\n",
    "        self.images_dir = os.path.join(self.root, 'leftImg8bit', split)\n",
    "        self.targets_dir = os.path.join(self.root, self.mode, split)\n",
    "        self.target_type = target_type\n",
    "        self.split = split\n",
    "        self.images = []\n",
    "        self.targets = []\n",
    "        self.albumAugs=albumAug()\n",
    "\n",
    "        verify_str_arg(mode, \"mode\", (\"fine\", \"coarse\"))\n",
    "        if mode == \"fine\":\n",
    "            valid_modes = (\"train\", \"test\", \"val\")\n",
    "        else:\n",
    "            valid_modes = (\"train\", \"train_extra\", \"val\")\n",
    "        msg = (\"Unknown value '{}' for argument split if mode is '{}'. \"\n",
    "               \"Valid values are {{{}}}.\")\n",
    "        msg = msg.format(split, mode, iterable_to_str(valid_modes))\n",
    "        verify_str_arg(split, \"split\", valid_modes, msg)\n",
    "\n",
    "        if not isinstance(target_type, list):\n",
    "            self.target_type = [target_type]\n",
    "        [verify_str_arg(value, \"target_type\",\n",
    "                        (\"instance\", \"semantic\", \"polygon\", \"color\"))\n",
    "         for value in self.target_type]\n",
    "\n",
    "        if not os.path.isdir(self.images_dir) or not os.path.isdir(self.targets_dir):\n",
    "\n",
    "            if split == 'train_extra':\n",
    "                image_dir_zip = os.path.join(self.root, 'leftImg8bit{}'.format('_trainextra.zip'))\n",
    "            else:\n",
    "                image_dir_zip = os.path.join(self.root, 'leftImg8bit{}'.format('_trainvaltest.zip'))\n",
    "\n",
    "            if self.mode == 'gtFine':\n",
    "                target_dir_zip = os.path.join(self.root, '{}{}'.format(self.mode, '_trainvaltest.zip'))\n",
    "            elif self.mode == 'gtCoarse':\n",
    "                target_dir_zip = os.path.join(self.root, '{}{}'.format(self.mode, '.zip'))\n",
    "\n",
    "            if os.path.isfile(image_dir_zip) and os.path.isfile(target_dir_zip):\n",
    "                extract_archive(from_path=image_dir_zip, to_path=self.root)\n",
    "                extract_archive(from_path=target_dir_zip, to_path=self.root)\n",
    "            else:\n",
    "                raise RuntimeError('Dataset not found or incomplete. Please make sure all required folders for the'\n",
    "                                   ' specified \"split\" and \"mode\" are inside the \"root\" directory')\n",
    "\n",
    "        for city in os.listdir(self.images_dir):\n",
    "            img_dir = os.path.join(self.images_dir, city)\n",
    "            target_dir = os.path.join(self.targets_dir, city)\n",
    "            for file_name in os.listdir(img_dir):\n",
    "                target_types = []\n",
    "                for t in self.target_type:\n",
    "                    target_name = '{}_{}'.format(file_name.split('_leftImg8bit')[0],\n",
    "                                                 self._get_target_suffix(self.mode, t))\n",
    "                    target_types.append(os.path.join(target_dir, target_name))\n",
    "\n",
    "                self.images.append(os.path.join(img_dir, file_name))\n",
    "                self.targets.append(target_types)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is a tuple of all target types if target_type is a list with more\n",
    "            than one item. Otherwise target is a json object if target_type=\"polygon\", else the image segmentation.\n",
    "        \"\"\"\n",
    "\n",
    "        image = Image.open(self.images[index]).convert('RGB')\n",
    "\n",
    "        targets = []\n",
    "        for i, t in enumerate(self.target_type):\n",
    "            if t == 'polygon':\n",
    "                target = self._load_json(self.targets[index][i])\n",
    "            else:\n",
    "                target = Image.open(self.targets[index][i])\n",
    "\n",
    "            targets.append(target)\n",
    "\n",
    "        target = tuple(targets) if len(targets) > 1 else targets[0]\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            #Turn image, label into numpy array\n",
    "            image = np.array(image).astype(np.uint8)\n",
    "            target = np.array(target).astype(np.uint8)\n",
    "            \n",
    "            #Do albumentations augmentations\n",
    "            data = {'image': image, 'mask':target}\n",
    "            augmented = self.albumAugs(**data)\n",
    "            image, target = augmented['image'], augmented['mask']\n",
    "            \n",
    "            #Convert back to PIL Image for pytorch augmentations\n",
    "            image, target = Image.fromarray(image), Image.fromarray(target)\n",
    "        \n",
    "        #Pytorch Augmentations\n",
    "        if self.transforms is not None:\n",
    "            image, target = self.transforms(image, target)\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        lines = [\"Split: {split}\", \"Mode: {mode}\", \"Type: {target_type}\"]\n",
    "        return '\\n'.join(lines).format(**self.__dict__)\n",
    "\n",
    "    def _load_json(self, path):\n",
    "        with open(path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "        return data\n",
    "\n",
    "    def _get_target_suffix(self, mode, target_type):\n",
    "        if target_type == 'instance':\n",
    "            return '{}_instanceIds.png'.format(mode)\n",
    "        elif target_type == 'semantic':\n",
    "            return '{}_labelIds.png'.format(mode)\n",
    "        elif target_type == 'color':\n",
    "            return '{}_color.png'.format(mode)\n",
    "        else:\n",
    "            return '{}_polygons.json'.format(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (GaussianBlur, RandomFog, HorizontalFlip, Resize, Compose)\n",
    "from albumentations.pytorch import ToTensor\n",
    "#Transforms\n",
    "def trainAugX():\n",
    "    p=0.5\n",
    "    return TF.Compose([\n",
    "        TF.ColorJitter(brightness=0.3, contrast=0.2, saturation=0.1, hue=0),\n",
    "#        GaussianBlur(blur_limit=7, always_apply=False, p=p),\n",
    "#        RandomFog(fog_coef_lower=0.3, fog_coef_upper=1, alpha_coef=0.08, always_apply=False, p=p),\n",
    "#        HorizontalFlip(always_apply=False, p=p),\n",
    "#        Resize(512, 1024, interpolation=0, always_apply=False, p=1),\n",
    "        TF.Resize(size = (512,1024), interpolation=0),\n",
    "        TF.ToTensor(),\n",
    "    ])\n",
    "def trainAugXY(p=0.5):\n",
    "    return TF.Compose([TF.RandomHorizontalFlip(p=0.5)])\n",
    "def trainElse():\n",
    "    return TF.Compose([TF.Resize(size = (512,1024), interpolation=0), TF.ToTensor(),])\n",
    "totensor = trainElse()\n",
    "trainTransformX = trainAugX()\n",
    "trainTransformXY = trainAugXY()\n",
    "\n",
    "#Read in datasets\n",
    "trainData = Cityscapes('../kruelle/kruelle', split='train', mode='fine', \n",
    "                    target_type='semantic', transform=trainTransformX, \n",
    "                              target_transform=totensor, transforms=None)\n",
    "valData = Cityscapes('../kruelle/kruelle', split='val', mode='fine', \n",
    "                    target_type='semantic', transform=totensor, \n",
    "                              target_transform=totensor, transforms=None)\n",
    "testData = Cityscapes('../kruelle/kruelle', split='test', mode='fine', \n",
    "                    target_type='semantic', transform=totensor, \n",
    "                               target_transform=totensor, transforms=None)\n",
    "\n",
    "#Put into dataloaders\n",
    "trainLoader = DataLoader(trainData, batch_size=8, shuffle=True)\n",
    "valLoader = DataLoader(valData, batch_size=5, shuffle=False)\n",
    "testLoader = DataLoader(testData, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('../kruelle/kruelle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TAKEN FROM CITYSCAPE GITHUB\n",
    "## https://github.com/mcordts/cityscapesScripts/blob/master/cityscapesscripts/helpers/labels.py\n",
    "\n",
    "from __future__ import print_function, absolute_import, division\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Definitions\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# a label and all meta information\n",
    "Label = namedtuple( 'Label' , [\n",
    "\n",
    "    'name'        , # The identifier of this label, e.g. 'car', 'person', ... .\n",
    "                    # We use them to uniquely name a class\n",
    "\n",
    "    'id'          , # An integer ID that is associated with this label.\n",
    "                    # The IDs are used to represent the label in ground truth images\n",
    "                    # An ID of -1 means that this label does not have an ID and thus\n",
    "                    # is ignored when creating ground truth images (e.g. license plate).\n",
    "                    # Do not modify these IDs, since exactly these IDs are expected by the\n",
    "                    # evaluation server.\n",
    "\n",
    "    'trainId'     , # Feel free to modify these IDs as suitable for your method. Then create\n",
    "                    # ground truth images with train IDs, using the tools provided in the\n",
    "                    # 'preparation' folder. However, make sure to validate or submit results\n",
    "                    # to our evaluation server using the regular IDs above!\n",
    "                    # For trainIds, multiple labels might have the same ID. Then, these labels\n",
    "                    # are mapped to the same class in the ground truth images. For the inverse\n",
    "                    # mapping, we use the label that is defined first in the list below.\n",
    "                    # For example, mapping all void-type classes to the same ID in training,\n",
    "                    # might make sense for some approaches.\n",
    "                    # Max value is 255!\n",
    "\n",
    "    'category'    , # The name of the category that this label belongs to\n",
    "\n",
    "    'categoryId'  , # The ID of this category. Used to create ground truth images\n",
    "                    # on category level.\n",
    "\n",
    "    'hasInstances', # Whether this label distinguishes between single instances or not\n",
    "\n",
    "    'ignoreInEval', # Whether pixels having this class as ground truth label are ignored\n",
    "                    # during evaluations or not\n",
    "\n",
    "    'color'       , # The color of this label\n",
    "    ] )\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# A list of all labels\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Please adapt the train IDs as appropriate for your approach.\n",
    "# Note that you might want to ignore labels with ID 255 during training.\n",
    "# Further note that the current train IDs are only a suggestion. You can use whatever you like.\n",
    "# Make sure to provide your results using the original IDs and not the training IDs.\n",
    "# Note that many IDs are ignored in evaluation and thus you never need to predict these!\n",
    "\n",
    "labels = [\n",
    "    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n",
    "    Label(  'unlabeled'            ,  0 ,       19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'ego vehicle'          ,  1 ,       19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'rectification border' ,  2 ,       19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'out of roi'           ,  3 ,       19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'static'               ,  4 ,       19 , 'void'            , 0       , False        , True         , (  0,  0,  0) ),\n",
    "    Label(  'dynamic'              ,  5 ,       19 , 'void'            , 0       , False        , True         , (111, 74,  0) ),\n",
    "    Label(  'ground'               ,  6 ,       19 , 'void'            , 0       , False        , True         , ( 81,  0, 81) ),\n",
    "    Label(  'road'                 ,  7 ,        0 , 'flat'            , 1       , False        , False        , (128, 64,128) ),\n",
    "    Label(  'sidewalk'             ,  8 ,        1 , 'flat'            , 1       , False        , False        , (244, 35,232) ),\n",
    "    Label(  'parking'              ,  9 ,       19 , 'flat'            , 1       , False        , True         , (250,170,160) ),\n",
    "    Label(  'rail track'           , 10 ,       19 , 'flat'            , 1       , False        , True         , (230,150,140) ),\n",
    "    Label(  'building'             , 11 ,        2 , 'construction'    , 2       , False        , False        , ( 70, 70, 70) ),\n",
    "    Label(  'wall'                 , 12 ,        3 , 'construction'    , 2       , False        , False        , (102,102,156) ),\n",
    "    Label(  'fence'                , 13 ,        4 , 'construction'    , 2       , False        , False        , (190,153,153) ),\n",
    "    Label(  'guard rail'           , 14 ,       19 , 'construction'    , 2       , False        , True         , (180,165,180) ),\n",
    "    Label(  'bridge'               , 15 ,       19 , 'construction'    , 2       , False        , True         , (150,100,100) ),\n",
    "    Label(  'tunnel'               , 16 ,       19 , 'construction'    , 2       , False        , True         , (150,120, 90) ),\n",
    "    Label(  'pole'                 , 17 ,        5 , 'object'          , 3       , False        , False        , (153,153,153) ),\n",
    "    Label(  'polegroup'            , 18 ,       19 , 'object'          , 3       , False        , True         , (153,153,153) ),\n",
    "    Label(  'traffic light'        , 19 ,        6 , 'object'          , 3       , False        , False        , (250,170, 30) ),\n",
    "    Label(  'traffic sign'         , 20 ,        7 , 'object'          , 3       , False        , False        , (220,220,  0) ),\n",
    "    Label(  'vegetation'           , 21 ,        8 , 'nature'          , 4       , False        , False        , (107,142, 35) ),\n",
    "    Label(  'terrain'              , 22 ,        9 , 'nature'          , 4       , False        , False        , (152,251,152) ),\n",
    "    Label(  'sky'                  , 23 ,       10 , 'sky'             , 5       , False        , False        , ( 70,130,180) ),\n",
    "    Label(  'person'               , 24 ,       11 , 'human'           , 6       , True         , False        , (220, 20, 60) ),\n",
    "    Label(  'rider'                , 25 ,       12 , 'human'           , 6       , True         , False        , (255,  0,  0) ),\n",
    "    Label(  'car'                  , 26 ,       13 , 'vehicle'         , 7       , True         , False        , (  0,  0,142) ),\n",
    "    Label(  'truck'                , 27 ,       14 , 'vehicle'         , 7       , True         , False        , (  0,  0, 70) ),\n",
    "    Label(  'bus'                  , 28 ,       15 , 'vehicle'         , 7       , True         , False        , (  0, 60,100) ),\n",
    "    Label(  'caravan'              , 29 ,       19 , 'vehicle'         , 7       , True         , True         , (  0,  0, 90) ),\n",
    "    Label(  'trailer'              , 30 ,       19 , 'vehicle'         , 7       , True         , True         , (  0,  0,110) ),\n",
    "    Label(  'train'                , 31 ,       16 , 'vehicle'         , 7       , True         , False        , (  0, 80,100) ),\n",
    "    Label(  'motorcycle'           , 32 ,       17 , 'vehicle'         , 7       , True         , False        , (  0,  0,230) ),\n",
    "    Label(  'bicycle'              , 33 ,       18 , 'vehicle'         , 7       , True         , False        , (119, 11, 32) ),\n",
    "    Label(  'license plate'        , -1 ,       -1 , 'vehicle'         , 7       , False        , True         , (  0,  0,142) ),\n",
    "]\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Create dictionaries for a fast lookup\n",
    "#--------------------------------------------------------------------------------\n",
    "\n",
    "# Please refer to the main method below for example usages!\n",
    "\n",
    "# name to label object\n",
    "name2label      = { label.name    : label for label in labels           }\n",
    "# id to label object\n",
    "id2label        = { label.id      : label for label in labels           }\n",
    "# trainId to label object\n",
    "trainId2label   = { label.trainId : label for label in reversed(labels) }\n",
    "# category to list of label objects\n",
    "category2labels = {}\n",
    "for label in labels:\n",
    "    category = label.category\n",
    "    if category in category2labels:\n",
    "        category2labels[category].append(label)\n",
    "    else:\n",
    "        category2labels[category] = [label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA VISUALIZATION\n",
    "\n",
    "dataiter = iter(trainLoader)\n",
    "imgs, lbls = dataiter.next()\n",
    "lbls = (lbls*255).type(torch.LongTensor)\n",
    "\n",
    "print(imgs.shape, lbls.shape)\n",
    "#print(lbls[0,0,1020,1000])\n",
    "\n",
    "for i in range(34):\n",
    "    lbls[lbls == i] = id2label[i].trainId+50\n",
    "lbls = lbls-50\n",
    "    \n",
    "#print(lbls[0,0,1020,1000])\n",
    "\n",
    "im2display = imgs[0].numpy().transpose((1,2,0))\n",
    "lb2display = torch.squeeze(lbls[0]).numpy()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols= 2, figsize=(16,10))\n",
    "ax1.imshow(im2display)\n",
    "ax2.imshow(lb2display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEFINE UNET MODEL\n",
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm2d(out_channels)\n",
    "    )   \n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.dconv_down1 = double_conv(3, 64)\n",
    "        self.dconv_down2 = double_conv(64, 128)\n",
    "        self.dconv_down3 = double_conv(128, 256)\n",
    "        self.dconv_down4 = double_conv(256, 512)        \n",
    "\n",
    "        self.maxpool4 = nn.MaxPool2d(4)\n",
    "        self.maxpool2 = nn.MaxPool2d(2)\n",
    "        self.firstdropout = nn.Dropout2d(p=0.125)\n",
    "        self.dropout = nn.Dropout2d(p=0.25)\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        self.upsample4 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)        \n",
    "        \n",
    "        self.dconv_up3 = double_conv(256 + 512, 256)\n",
    "        self.dconv_up2 = double_conv(128 + 256, 128)\n",
    "        self.dconv_up1 = double_conv(128 + 64, 64)\n",
    "        \n",
    "        self.conv_last = nn.Conv2d(64, n_class, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        conv1 = self.dconv_down1(x)\n",
    "        x = self.maxpool4(conv1)\n",
    "        x = self.firstdropout(x)\n",
    "\n",
    "        conv2 = self.dconv_down2(x)\n",
    "        x = self.maxpool4(conv2)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        conv3 = self.dconv_down3(x)\n",
    "        x = self.maxpool2(conv3)  \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dconv_down4(x)\n",
    "        \n",
    "        x = self.upsample2(x)        \n",
    "        x = torch.cat([x, conv3], dim=1)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample4(x)        \n",
    "        x = torch.cat([x, conv2], dim=1)   \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample4(x)        \n",
    "        x = torch.cat([x, conv1], dim=1)   \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.dconv_up1(x)\n",
    "        \n",
    "        out = self.conv_last(x)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy (IoU) Metric\n",
    "def iou(pred, target, n_classes = 20):\n",
    "  ious = []\n",
    "  pred = pred.view(-1)\n",
    "  target = target.view(-1)\n",
    "\n",
    "  for cls in range(n_classes):  \n",
    "    pred_inds = pred == cls\n",
    "    target_inds = target == cls\n",
    "    intersection = (pred_inds[target_inds]).long().sum().data.cpu()  # Cast to long to prevent overflows\n",
    "    union = pred_inds.long().sum().data.cpu() + target_inds.long().sum().data.cpu() - intersection\n",
    "    if union == 0:\n",
    "      ious.append(float('nan'))  # If there is no ground truth, do not include in evaluation\n",
    "    else:\n",
    "      ious.append(float(intersection) / float(max(union, 1)))\n",
    "  return np.array(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE UNET MODEL\n",
    "model = UNet(20)\n",
    "device = 'cuda:0'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKE DEEPLABV3 MODEL\n",
    "model = models.segmentation.deeplabv3_resnet50(pretrained=False, progress=True, num_classes=20, aux_loss=None)\n",
    "device = 'cuda:0'\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OR LOAD MODEL\n",
    "#model_name = 'CityscapeUnetModel'\n",
    "#model_name = 'CityscapesDeeplabModel'\n",
    "model_name = 'CityscapesDeeplabModel2'\n",
    "model = torch.load(model_name)\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN MODEL\n",
    "#model_name = 'CityscapeUnetModel'\n",
    "model_name = 'CityscapesDeeplabModel2'\n",
    "val_loss_least = 1000\n",
    "model.train()# put model in train mode\n",
    "criterion = torch.nn.CrossEntropyLoss() #Define loss function\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum=0.99) #define backpropogation function\n",
    "#optimizer = optim.RMSprop(model.parameters(),lr=0.01,alpha=0.99) # Define the learning function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "#optimizer = optim.Adamax(model.parameters(), lr=0.002, betas=(0.9, 0.999), eps=1e-08, weight_decay=0) #10\n",
    "#optimizer = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01, amsgrad=False)\n",
    "epochs = 20 #Num epochs\n",
    "time0 = time() #initialize time\n",
    "for e in range(epochs): #Train over e epochs\n",
    "    running_loss=0 #Init running loss for train data\n",
    "    running_val_loss=0 #Init running loss for validation data\n",
    "#    preds_test = torch.zeros((2975,1024,2048)) #For use in IoU accuracy of test data\n",
    "#    gts_test = torch.zeros((2975,1024,2048)) #For use in IoU accuracy of test data\n",
    "    preds_val = torch.zeros((500,512,1024)) #For use in IoU accuracy of validation data\n",
    "    gts_val = torch.zeros((500,512,1024)) #For use in IoU accuracy of validation data #20\n",
    "    model.train() # Set model to train mode\n",
    "    for i,(x,y) in enumerate(trainLoader): #Do for each batch in dataloader\n",
    "        x = x.to(device) # Send inputs to GPU\n",
    "        y = torch.squeeze((y*255).type(torch.LongTensor))\n",
    "#        gts_test[i:i+2, :, :] = y #copy y data for use in IoU\n",
    "        y = y.to(device) #send labels to GPU\n",
    "        for k in range(34):\n",
    "            y[y == k] = id2label[k].trainId+50\n",
    "        y = y-50 #30\n",
    "        if (y.shape[0] == 1024):\n",
    "            y = torch.unsqueeze(y, 0)\n",
    "        #print(output.shape, y.shape)\n",
    "        optimizer.zero_grad() #Zero the gradients on the optimizer\n",
    "        output = model(x)['out'] #Get output from the model\n",
    "        loss = criterion(output,y) #Calculate loss\n",
    "        loss.backward() #Backpropogation on loss\n",
    "        optimizer.step() #Learn from loss\n",
    "#        _,pred = torch.max(output, 1) #Get output in correct format for IoU\n",
    "#        preds_test[i:i+2, :, :] = pred.cpu() #Send back to CPU and copy for use in IoU #40\n",
    "        writer.add_scalar('loss/training_per_minibatch', loss.item(), 2975/8*(e)+i) #Write loss to tensorboard per minibatch\n",
    "        running_loss+=loss.item() #Track runningloss for epoch avg\n",
    "        if (i%25 == 24): print(\"{}%\".format((100*(i+1))//(2975//(8))))\n",
    "    print(\"Epoch {} - Training loss: {}\".format(e,running_loss/(2975/8))) #Prints epoch running loss\n",
    "    writer.add_scalar('loss/training_per_epoch', running_loss/(2975/8), e) #Write epoch avg loss to tensorboard\n",
    "    print(\"        - Time Since Start (in minutes) = \",(time()-time0)/60)\n",
    "    model.eval() # Set model to eval mode\n",
    "    for j,(x,y) in enumerate(valLoader): #Do for each batch in dataloader\n",
    "        x = x.to(device) #Send inputs to GPU\n",
    "        y = torch.squeeze((y*255).type(torch.LongTensor))\n",
    "        y = y.to(device) #Send labels to device\n",
    "        for k in range(34):\n",
    "            y[y == k] = id2label[k].trainId+50\n",
    "        y = y-50\n",
    "        if (y.shape[0] == 1024):\n",
    "            y = torch.unsqueeze(y, 0)\n",
    "        gts_val[2*j:2*(j+1), :, :] = y.cpu() #copy labels for use in IoU\n",
    "        optimizer.zero_grad() #Zero the gradients\n",
    "        output = model(x)['out'] #Get output from model\n",
    "        loss = criterion(output,y) #Calculate the loss\n",
    "        _,pred = torch.max(output, 1) #Get output in correct format for IoU use\n",
    "        preds_val[2*j:2*(j+1), :, :] = pred.cpu() #Send back to CPU and copy for use in IoU\n",
    "        running_val_loss+=loss.item() #Keep track of running loss for epoch avg\n",
    "    writer.add_scalar('loss/validation_per_epoch',running_val_loss/(500/2),e) #Write epoch avg loss to tensorboard\n",
    "#    test_acc = iou(preds_test,gts_test) # Get IoU accuracy for test data\n",
    "#    test_acc_avg = np.average(test_acc) #Avgerage it over digit classes\n",
    "#    writer.add_scalar('accuracy/training', test_acc_avg, e) #Write it to tensorboard\n",
    "    print(\"        - Validation Loss: {}\".format(running_val_loss/(500/2))) \n",
    "    print(\"        - Time Since Start (in minutes) = \",(time()-time0)/60)\n",
    "    val_acc = iou(preds_val,gts_val) #Same thing as above\n",
    "    val_acc_avg = np.average(val_acc)\n",
    "    writer.add_scalar('accuracy/validation', val_acc_avg, e)\n",
    "#    print(\"        - Training Accuracy: {}\".format(test_acc_avg))\n",
    "    print(\"        - Validation Accuracy: {}\".format(val_acc_avg))\n",
    "    print(\"        - Time Since Start (in minutes) = \",(time()-time0)/60)\n",
    "    if (running_val_loss/(500/2) < val_loss_least):\n",
    "        torch.save(model,model_name)\n",
    "        val_loss_least = running_val_loss/(500/2)\n",
    "        \n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60) #Prints total training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Epoch {} - Training loss: {}\".format(e,running_loss/(i))) #Prints epoch running loss\n",
    "print(\"        - Validation Loss: {}\".format(running_val_loss/(500/1))) \n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60) #Prints total training time\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in range(2975):\n",
    "    if(gts_test[i,0,0] != 0):\n",
    "        count+=1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVING THE MODEL:\n",
    "model_name = 'CityscapeUnetModel'\n",
    "torch.save(model,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cuda:0'\n",
    "#model.to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "running_test_loss=0\n",
    "preds_val = torch.zeros((500,512,1024)) #For use in IoU accuracy of validation data\n",
    "gts_val = torch.zeros((500,512,1024)) #For use in IoU accuracy of validation data #20\n",
    "time0 = time()\n",
    "model.eval() #Put model in evaluate mode\n",
    "for j,(x,y) in enumerate(valLoader): #Do for each batch in dataloader\n",
    "        x = x.to(device) #Send inputs to GPU\n",
    "        y = torch.squeeze((y*255).type(torch.LongTensor))\n",
    "        y = y.to(device) #Send labels to device\n",
    "        for k in range(34):\n",
    "            y[y == k] = id2label[k].trainId+50\n",
    "        y = y-50\n",
    "        if (y.shape[0] == 1024):\n",
    "            y = torch.unsqueeze(y, 0)\n",
    "        gts_val[5*j:5*(j+1), :, :] = y.cpu() #copy labels for use in IoU\n",
    "        optimizer.zero_grad() #Zero the gradients\n",
    "        output = model(x)['out'] #Get output from model\n",
    "        loss = criterion(output,y) #Calculate the loss\n",
    "        _,pred = torch.max(output, 1) #Get output in correct format for IoU use\n",
    "        preds_val[5*j:5*(j+1), :, :] = pred.cpu() #Send back to CPU and copy for use in IoU\n",
    "        running_test_loss+=loss.item() #Keep track of running loss for epoch avg\n",
    "print(\"Test Loss: {}\".format(running_test_loss/(500/5)))\n",
    "print(\"Time so far: {}\".format(-(time0-time())/60))\n",
    "test_accuracy = iou(preds_val, gts_val)\n",
    "avg_test_acc = np.average(test_accuracy)\n",
    "print(\"Test Accuracy: {}\".format(avg_test_acc))\n",
    "print(\"Time: {}\".format(-(time0-time())/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIter = iter(valLoader)\n",
    "\n",
    "imgs, lbls = testIter.next()\n",
    "\n",
    "print(imgs.shape, lbls.shape)\n",
    "\n",
    "imgs = imgs.to(device)\n",
    "output = model(imgs)['out']\n",
    "_, prediction = torch.max(output, 1)\n",
    "print(output.shape, prediction.shape)\n",
    "lbls = torch.squeeze((lbls*255).type(torch.LongTensor))\n",
    "for k in range(34):\n",
    "    lbls[lbls == k] = id2label[k].trainId+50\n",
    "lbls = lbls-50\n",
    "if (lbls.shape[0] == 1024):\n",
    "    lbls = torch.unsqueeze(lbls, 0)\n",
    "\n",
    "\n",
    "im2display = imgs.cpu().numpy().transpose((0,2,3,1))\n",
    "pred2display = prediction.detach().cpu().numpy()\n",
    "lbl2display = lbls.numpy()\n",
    "\n",
    "del imgs\n",
    "del output\n",
    "del prediction\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "#print(lb2display[200,300])\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(nrows=1, ncols= 2, figsize=(16,10))\n",
    "#ax1.imshow(im2display)\n",
    "#ax2.imshow(lb2display)\n",
    "\n",
    "fig, axs = plt.subplots(5, 3, figsize=(16,20))\n",
    "axs[0,0].imshow(im2display[0])\n",
    "axs[0,0].set_title('Original Image')\n",
    "axs[0,1].imshow(pred2display[0])\n",
    "axs[0,1].set_title('Predicted Labels')\n",
    "axs[0,2].imshow(lbl2display[0])\n",
    "axs[0,2].set_title('Target Labels')\n",
    "axs[1,0].imshow(im2display[1])\n",
    "axs[1,1].imshow(pred2display[1])\n",
    "axs[1,2].imshow(lbl2display[1])\n",
    "axs[2,0].imshow(im2display[2])\n",
    "axs[2,1].imshow(pred2display[2])\n",
    "axs[2,2].imshow(lbl2display[2])\n",
    "axs[3,0].imshow(im2display[3])\n",
    "axs[3,1].imshow(pred2display[3])\n",
    "axs[3,2].imshow(lbl2display[3])\n",
    "axs[4,0].imshow(im2display[4])\n",
    "axs[4,1].imshow(pred2display[4])\n",
    "axs[4,2].imshow(lbl2display[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
